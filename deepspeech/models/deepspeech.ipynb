{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "import psutil\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "from deepspeech.data import preprocess\n",
    "from deepspeech.models.model import Model\n",
    "from deepspeech.networks.deepspeech import Network\n",
    "\n",
    "from deepspeech.data.alphabet import Alphabet\n",
    "from deepspeech.decoder import GreedyCTCDecoder\n",
    "from deepspeech.global_state import GlobalState\n",
    "from deepspeech.logging import log_call_info\n",
    "from deepspeech.logging import LoggerMixin\n",
    "from deepspeech.loss import CTCLoss\n",
    "from deepspeech.loss import levenshtein\n",
    "from deepspeech.networks.utils import to_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_BLANK_SYMBOL = '_'\n",
    "\n",
    "\n",
    "def _gen_alphabet():\n",
    "    symbols = list(\" abcdefghijklmnopqrstuvwxyz'\")\n",
    "    symbols.append(_BLANK_SYMBOL)\n",
    "    return Alphabet(symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(LoggerMixin):\n",
    "    \"\"\"A speech-to-text model.\n",
    "\n",
    "    Args:\n",
    "        network: A speech-to-text `torch.nn.Module`.\n",
    "        optimiser_cls (callable, optional): If not None, this optimiser will be\n",
    "            instantiated with an OrderedDict of the network parameters as the\n",
    "            first argument and **optimiser_kwargs as the remaining arguments\n",
    "            unless they are None.\n",
    "        optimiser_kwargs (dict, optional): A dictionary of arguments to pass to\n",
    "            the optimiser when it is created. Defaults to the empty dictionary\n",
    "            if None.\n",
    "        decoder_cls (callable, optional): A callable that implements the\n",
    "            `deepspeech.decoder.Decoder` interface. Defaults to\n",
    "            `DEFAULT_DECODER_CLS` if None.\n",
    "        decoder_kwargs (dict): A dictionary of arguments to pass to the decoder\n",
    "            when it is created. Defaults to `DEFAULT_DECODER_KWARGS` if\n",
    "            `decoder_kwargs` is None.\n",
    "        clip_gradients (int, optional): If None no gradient clipping is\n",
    "            performed. If an int, it is used as the `max_norm` parameter to\n",
    "            `torch.nn.utils.clip_grad_norm`.\n",
    "\n",
    "    Attributes:\n",
    "        BLANK_SYMBOL: The string that denotes the blank symbol in the CTC\n",
    "            algorithm.\n",
    "        ALPHABET: A `deepspeech.data.alphabet.Alphabet` - contains\n",
    "            `BLANK_SYMBOL`.\n",
    "        DEFAULT_DECODER_CLS: See Args.\n",
    "        DEFAULT_DECODER_KWARGS: See Args.\n",
    "        completed_epochs: Number of epochs completed during training.\n",
    "        network: See Args.\n",
    "        decoder: A `deepspeech.decoder.BeamCTCDecoder` instance.\n",
    "        optimiser: An `optimiser_cls` instance or None.\n",
    "        loss: A `deepspeech.loss.CTCLoss` instance.\n",
    "        transform: A function that returns a transformed piece of audio data.\n",
    "        target_transform: A function that returns a transformed target.\n",
    "    \"\"\"\n",
    "    BLANK_SYMBOL = _BLANK_SYMBOL\n",
    "    ALPHABET = _gen_alphabet()\n",
    "\n",
    "    DEFAULT_DECODER_CLS = GreedyCTCDecoder\n",
    "    DEFAULT_DECODER_KWARGS = {'alphabet': ALPHABET,\n",
    "                              'blank_symbol': BLANK_SYMBOL}\n",
    "\n",
    "    def __init__(self, network, optimiser_cls=None, optimiser_kwargs=None,\n",
    "                 decoder_cls=None, decoder_kwargs=None, clip_gradients=None):\n",
    "        self.completed_epochs = 0\n",
    "\n",
    "        self._optimiser_cls = optimiser_cls\n",
    "        self._optimiser_kwargs = optimiser_kwargs\n",
    "        self._clip_gradients = clip_gradients\n",
    "\n",
    "        self._init_network(network)\n",
    "        self._init_decoder(decoder_cls, decoder_kwargs)\n",
    "        self._init_optimiser()\n",
    "        self._init_loss()\n",
    "\n",
    "        self._global_state = GlobalState.get_or_init_singleton()\n",
    "\n",
    "    def _init_network(self, network):\n",
    "        if not torch.cuda.is_available():\n",
    "            self._logger.info('CUDA not available')\n",
    "        else:\n",
    "            self._logger.info('CUDA available, moving network '\n",
    "                              'parameters and buffers to the GPU')\n",
    "            to_cuda(network)\n",
    "\n",
    "        self.network = network\n",
    "\n",
    "    def _init_decoder(self, decoder_cls, decoder_kwargs):\n",
    "        if decoder_cls is None:\n",
    "            decoder_cls = self.DEFAULT_DECODER_CLS\n",
    "\n",
    "        if decoder_kwargs is None:\n",
    "            decoder_kwargs = copy.copy(self.DEFAULT_DECODER_KWARGS)\n",
    "\n",
    "        self.decoder = decoder_cls(**decoder_kwargs)\n",
    "\n",
    "    def _init_optimiser(self):\n",
    "        self.reset_optimiser()\n",
    "\n",
    "    def reset_optimiser(self):\n",
    "        \"\"\"Assigns a new `self.optimiser` using the current network params.\"\"\"\n",
    "        if self._optimiser_cls is None:\n",
    "            self.optimiser = None\n",
    "            self._logger.debug('No optimiser specified')\n",
    "            return\n",
    "\n",
    "        kwargs = self._optimiser_kwargs or {}\n",
    "        opt = self._optimiser_cls(self.network.parameters(), **kwargs)\n",
    "\n",
    "        self.optimiser = opt\n",
    "\n",
    "    def _init_loss(self):\n",
    "        blank_index = self.ALPHABET.get_index(self.BLANK_SYMBOL)\n",
    "        self.loss = CTCLoss(blank_index=blank_index,\n",
    "                            size_average=False,\n",
    "                            length_average=False)\n",
    "\n",
    "    @property\n",
    "    def transform(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def target_transform(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def state_dict(self):\n",
    "        state = {'completed_epochs': self.completed_epochs,\n",
    "                 'network': self.network.state_dict(),\n",
    "                 'global_state': self._global_state.state_dict()}\n",
    "        if self.optimiser is not None:\n",
    "            state['optimiser'] = self.optimiser.state_dict()\n",
    "        return state\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.completed_epochs = state_dict['completed_epochs']\n",
    "        self.network.load_state_dict(state_dict['network'])\n",
    "        self._global_state.load_state_dict(state_dict['global_state'])\n",
    "        if self.optimiser is not None:\n",
    "            self.optimiser.load_state_dict(state_dict['optimiser'])\n",
    "\n",
    "    @property\n",
    "    def _zero_grad(self):\n",
    "        return lambda: self.network.zero_grad()\n",
    "\n",
    "    @property\n",
    "    def _backward(self):\n",
    "        return lambda batch_loss: batch_loss.backward()\n",
    "\n",
    "    @property\n",
    "    def _maybe_clip_gradients(self):\n",
    "        if self._clip_gradients is None:\n",
    "            return lambda: None\n",
    "\n",
    "        return lambda: clip_grad_norm_(self.network.parameters(),\n",
    "                                       self._clip_gradients)\n",
    "\n",
    "    @log_call_info\n",
    "    def train(self, loader):\n",
    "        \"\"\"Trains the Model for an epoch.\n",
    "\n",
    "        Args:\n",
    "            loader: A `torch.utils.data.DataLoader` that generates batches of\n",
    "                training data.\n",
    "        \"\"\"\n",
    "        if self.optimiser is None:\n",
    "            raise AttributeError('Cannot train when optimiser is None!')\n",
    "\n",
    "        self.network.train()\n",
    "        self._train_log_init()\n",
    "        epoch_loss = 0.0\n",
    "        total_samples = 0\n",
    "\n",
    "        data_iter = iter(loader)   # Explicit creation to log queue sizes.\n",
    "        for step, ((x, logit_lens), y) in enumerate(data_iter):\n",
    "            self._zero_grad()\n",
    "\n",
    "            logits = self.network(x)\n",
    "\n",
    "            batch_loss = self.loss(logits, y, logit_lens)\n",
    "\n",
    "            epoch_loss += batch_loss.item()\n",
    "\n",
    "            total_samples += len(logit_lens)\n",
    "\n",
    "            self._backward(batch_loss)\n",
    "\n",
    "            self._maybe_clip_gradients()\n",
    "\n",
    "            self.optimiser.step()\n",
    "\n",
    "            self._train_log_step(step, x, logits, logit_lens, batch_loss.item(), data_iter)  # noqa: E501\n",
    "\n",
    "            self._global_state.step += 1\n",
    "\n",
    "            del logits, x, logit_lens, y\n",
    "\n",
    "        self._train_log_end(epoch_loss, total_samples)\n",
    "        self.completed_epochs += 1\n",
    "\n",
    "    @log_call_info\n",
    "    def eval_wer(self, loader):\n",
    "        \"\"\"Evaluates the WER of the Model.\n",
    "\n",
    "        Args:\n",
    "            loader: A `torch.utils.data.DataLoader` that generates batches of\n",
    "                data.\n",
    "        \"\"\"\n",
    "        self.network.eval()\n",
    "\n",
    "        total_lev = 0\n",
    "        total_lab_len = 0\n",
    "        n = 0\n",
    "\n",
    "        self._logger.debug('idx,model_label_prediction,target,edit_distance')\n",
    "        for i, ((x, logit_lens), y) in enumerate(loader):\n",
    "            with torch.no_grad():   # Ensure the gradient isn't computed.\n",
    "                logits = self.network(x)\n",
    "\n",
    "            preds = self.decoder.decode(logits.cpu(), logit_lens)\n",
    "            acts = [''.join(self.ALPHABET.get_symbols(yi.data.numpy()))\n",
    "                    for yi in y]\n",
    "\n",
    "            for pred, act in zip(preds, acts):\n",
    "                lev = levenshtein(pred.split(), act.split())\n",
    "\n",
    "                self._logger.debug('%d,%r,%r,%d', n, pred, act, lev)\n",
    "\n",
    "                n += 1\n",
    "                total_lev += lev\n",
    "                total_lab_len += len(act.split())\n",
    "\n",
    "        wer = float(total_lev) / total_lab_len\n",
    "        self._logger.debug('eval/wer: %r', wer)\n",
    "        self._global_state.writer.add_scalar('eval/wer',\n",
    "                                             wer, self._global_state.step)\n",
    "\n",
    "        return wer\n",
    "\n",
    "    @log_call_info\n",
    "    def eval_loss(self, loader):\n",
    "        \"\"\"Evaluates the CTC loss of the Model.\n",
    "\n",
    "        Args:\n",
    "            loader: A `torch.utils.data.DataLoader` that generates batches of\n",
    "                data.\n",
    "        \"\"\"\n",
    "        self.network.eval()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        total_samples = 0\n",
    "\n",
    "        self._logger.debug('idx,batch_mean_sample_loss')\n",
    "\n",
    "        for i, ((x, logit_lens), y) in enumerate(loader):\n",
    "            with torch.no_grad():   # Ensure the gradient isn't computed.\n",
    "                logits = self.network(x)\n",
    "\n",
    "                batch_loss = self.loss(logits, y, logit_lens).item()\n",
    "                batch_samples = len(logit_lens)\n",
    "\n",
    "                total_loss += batch_loss\n",
    "                total_samples += batch_samples\n",
    "\n",
    "            self._logger.debug('%d,%f', i, batch_loss / batch_samples)\n",
    "\n",
    "        mean_sample_loss = total_loss / max(1, total_samples)\n",
    "        self._logger.debug('eval/mean_sample_loss: %f', mean_sample_loss)\n",
    "        self._global_state.writer.add_scalar('eval/mean_sample_loss',\n",
    "                                             mean_sample_loss,\n",
    "                                             self._global_state.step)\n",
    "        return mean_sample_loss\n",
    "\n",
    "    def _train_log_init(self):\n",
    "        header = 'step,global_step,completed_epochs,sum_logit_lens,loss'\n",
    "        self._logger.debug(header)\n",
    "        self._cum_batch_size = 0\n",
    "\n",
    "    def _train_log_step(self, step, x, logits, logit_lens, loss, data_iter):\n",
    "        start = time.time()\n",
    "\n",
    "        total_steps = logit_lens.sum().item()\n",
    "\n",
    "        self._logger.debug('%d,%d,%d,%d,%f',\n",
    "                           step,\n",
    "                           self._global_state.step,\n",
    "                           self.completed_epochs,\n",
    "                           total_steps,\n",
    "                           loss)\n",
    "\n",
    "        self._global_state.writer.add_scalar('train/batch_loss',\n",
    "                                             loss,\n",
    "                                             self._global_state.step)\n",
    "        self._global_state.writer.add_scalar('train/batch_size',\n",
    "                                             len(logit_lens),\n",
    "                                             self._global_state.step)\n",
    "\n",
    "        self._cum_batch_size += len(logit_lens)\n",
    "        self._global_state.writer.add_scalar('train/epoch_cum_batch_size',\n",
    "                                             self._cum_batch_size,\n",
    "                                             self._global_state.step)\n",
    "\n",
    "        self._global_state.writer.add_scalar('train/batch_len-x-batch_size',\n",
    "                                             x.size(0) * x.size(1),\n",
    "                                             self._global_state.step)\n",
    "        self._global_state.writer.add_scalar('train/sum_logit_lens',\n",
    "                                             total_steps,\n",
    "                                             self._global_state.step)\n",
    "        self._global_state.writer.add_scalar('train/memory_percent',\n",
    "                                             psutil.Process().memory_percent(),\n",
    "                                             self._global_state.step)\n",
    "\n",
    "        self._train_log_step_data_queue(data_iter)\n",
    "\n",
    "        self._train_log_step_cuda_memory()\n",
    "\n",
    "        self._train_log_step_grad_param_stats()\n",
    "\n",
    "        self._global_state.writer.add_scalar('train/log_step_time',\n",
    "                                             time.time() - start,\n",
    "                                             self._global_state.step)\n",
    "\n",
    "    def _train_log_step_data_queue(self, data_iter):\n",
    "        \"\"\"Logs the number of batches in the PyTorch DataLoader queue.\"\"\"\n",
    "        # If num_workers is 0 then there is no Queue and each batch is loaded\n",
    "        # when next is called.\n",
    "        if data_iter.num_workers > 0:\n",
    "            # Otherwise there exists a queue from which samples are read from.\n",
    "            if data_iter.pin_memory or data_iter.timeout > 0:\n",
    "                # The loader iterator in PyTorch 0.4 with pin_memory or a\n",
    "                # timeout has a single thread fill a queue.Queue from a\n",
    "                # multiprocessing.SimpleQueue that is filled by num_workers\n",
    "                # other workers. The queue.Queue is used when next is called.\n",
    "                # See: https://pytorch.org/docs/0.4.0/_modules/torch/utils/data/dataloader.html#DataLoader   # noqa: E501\n",
    "                self._global_state.writer.add_scalar(\n",
    "                    'train/queue_size',\n",
    "                    data_iter.data_queue.qsize(),\n",
    "                    self._global_state.step)\n",
    "            else:\n",
    "                # Otherwise the loader iterator reads from a\n",
    "                # multiprocessing.SimpleQueue. This has no size function...\n",
    "                self._global_state.writer.add_scalar(\n",
    "                    'train/queue_empty',\n",
    "                    data_iter.data_queue.empty(),\n",
    "                    self._global_state.step)\n",
    "\n",
    "    def _train_log_step_cuda_memory(self):\n",
    "        \"\"\"Logs CUDA memory usage.\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            self._global_state.writer.add_scalar(\n",
    "                'train/memory_allocated',\n",
    "                torch.cuda.memory_allocated(),\n",
    "                self._global_state.step)\n",
    "            self._global_state.writer.add_scalar(\n",
    "                'train/max_memory_allocated',\n",
    "                torch.cuda.max_memory_allocated(),\n",
    "                self._global_state.step)\n",
    "            self._global_state.writer.add_scalar(\n",
    "                'train/memory_cached',\n",
    "                torch.cuda.memory_cached(),\n",
    "                self._global_state.step)\n",
    "            self._global_state.writer.add_scalar(\n",
    "                'train/max_memory_cached',\n",
    "                torch.cuda.max_memory_cached(),\n",
    "                self._global_state.step)\n",
    "\n",
    "    def _train_log_step_grad_param_stats(self):\n",
    "        \"\"\"Logs gradient and parameter values.\"\"\"\n",
    "        if self._global_state.log_step():\n",
    "            for name, param in self.network.named_parameters():\n",
    "                self._global_state.writer.add_histogram(\n",
    "                    'parameters/%s' % name, param, self._global_state.step)\n",
    "\n",
    "                self._global_state.writer.add_histogram(\n",
    "                    'gradients/%s' % name, param.grad, self._global_state.step)\n",
    "\n",
    "    def _train_log_end(self, epoch_loss, total_samples):\n",
    "        mean_sample_loss = float(epoch_loss) / total_samples\n",
    "        self._logger.debug('train/mean_sample_loss: %r', mean_sample_loss)\n",
    "        self._logger.info('epoch %d finished', self.completed_epochs)\n",
    "\n",
    "        self._global_state.writer.add_scalar('train/mean_sample_loss',\n",
    "                                             mean_sample_loss,\n",
    "                                             self._global_state.step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSpeech(Model):\n",
    "    \"\"\"Deep Speech Model.\n",
    "\n",
    "    Args:\n",
    "        optimiser_cls: See `Model`.\n",
    "        optimiser_kwargs: See `Model`.\n",
    "        decoder_cls: See `Model`.\n",
    "        decoder_kwargs: See `Model`.\n",
    "        n_hidden (int): Internal hidden unit size.\n",
    "        n_context (int): Number of context frames to use on each side of the\n",
    "            current input frame.\n",
    "        n_mfcc (int): Number of Mel-Frequency Cepstral Coefficients to use as\n",
    "            input for a single frame.\n",
    "        drop_prob (float): Dropout drop probability, [0.0, 1.0] inclusive.\n",
    "        winlen (float): Window length in ms to compute input features over.\n",
    "        winstep (float): Window step size in ms.\n",
    "        sample_rate (int): Sample rate in Hz of input data.\n",
    "\n",
    "    Attributes:\n",
    "        See base class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimiser_cls=None, optimiser_kwargs=None,\n",
    "                 decoder_cls=None, decoder_kwargs=None,\n",
    "                 n_hidden=2048, n_context=9, n_mfcc=26, drop_prob=0.25,\n",
    "                 winlen=0.025, winstep=0.02, sample_rate=16000):\n",
    "\n",
    "        self._n_hidden = n_hidden\n",
    "        self._n_context = n_context\n",
    "        self._n_mfcc = n_mfcc\n",
    "        self._drop_prob = drop_prob\n",
    "        self._winlen = winlen\n",
    "        self._winstep = winstep\n",
    "        self._sample_rate = sample_rate\n",
    "\n",
    "        network = self._get_network()\n",
    "\n",
    "        super().__init__(network=network,\n",
    "                         optimiser_cls=optimiser_cls,\n",
    "                         optimiser_kwargs=optimiser_kwargs,\n",
    "                         decoder_cls=decoder_cls,\n",
    "                         decoder_kwargs=decoder_kwargs,\n",
    "                         clip_gradients=None)\n",
    "\n",
    "    def _get_network(self):\n",
    "        return Network(in_features=self._n_mfcc*(2*self._n_context + 1),\n",
    "                       n_hidden=self._n_hidden,\n",
    "                       out_features=len(self.ALPHABET),\n",
    "                       drop_prob=self._drop_prob)\n",
    "\n",
    "    @property\n",
    "    def transform(self):\n",
    "        return Compose([preprocess.MFCC(self._n_mfcc),\n",
    "                        preprocess.AddContextFrames(self._n_context),\n",
    "                        preprocess.Normalize(),\n",
    "                        torch.FloatTensor,\n",
    "                        lambda t: (t, len(t))])\n",
    "\n",
    "    @property\n",
    "    def target_transform(self):\n",
    "        return Compose([str.lower,\n",
    "                        self.ALPHABET.get_indices,\n",
    "                        torch.IntTensor])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
